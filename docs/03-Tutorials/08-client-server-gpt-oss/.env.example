# Local LLM Configuration
LLM_PROVIDER=ollama  # Options: ollama, transformers
MODEL_NAME=smollm:135m  # For Ollama: lightweight and fast model
# MODEL_NAME=google/flan-t5-base  # For Transformers - better for conversations

# Ollama Configuration (if using Ollama)
OLLAMA_HOST=http://localhost:11434
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama

# Server Configuration
STORAGE_MCP_SERVER_URL=http://localhost:8000/sse
